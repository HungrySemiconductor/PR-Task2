# 模式识别实验报告

**姓　　名**：范红乐  
**班　　级**：计算机学院20250718班  
**学　　号**：2025E8007382043

## 1 实验概述

### 1.1 实验内容

1. 实现两个通用的三层前向神经网络反向传播算法程序，一个采用批量方式更新权重，另一个采用单样本方式更新权重
2. 隐含层节点激励函数：双曲正切函数；输出层激励函数：Sigmoid函数；目标函数：平方误差准则函数

### 1.2 实验数据





## 2 算法实现

### 2.1 







## 3 实验步骤

### 3.1 目录结构

```
├── main.py  
├── dta_loader.py		# 数据预处理
├── plot_results.py		# 结果绘图处理
|
├── exp/             		 
│   ├── exp1_hidden_size.py   # 实验1 隐含层节点数测试
│   ├── exp2_learning_rate.py # 实验2 学习率测试
│   ├── exp3_update_method.py # 实验3 单样本更新 vs 批量更新
│   └── experiment_runner.py        
│
└── results/        
    ├── exp1/
    ├── exp2/
    └── exp3/
```

### 3.2 实验流程

#### 3.2.1 数据预处理



#### 3.2.2 网络结构设计

| 网络层 | 节点数        | 激活函数                                                     |
| ------ | ------------- | ------------------------------------------------------------ |
| 输入层 | 3（3个特征）  |                                                              |
| 隐含层 | a（数目可调） | 双曲正切： $f(x) = tanh(x) = \dfrac{e^x-e^{-x}}{e^x + e^{-x}}$ |
| 输出层 | 3（3个类别）  | Sigmoid： $f(x) = \dfrac{1}{1+e^{-x}}$                       |
|        | **损失函数**  | **平方误差**： $E = \dfrac{1}{2}\sum_{j=1}^{3}(t_j-z_j)^2$   |

#### 3.2.3 前向传播

1. 隐含层输入：$net_h = \sum_iw_{ih}x_i$  	

   隐含层输出：$y_h = tanh(net_h)$

2. 输出层输入：$net_j = \sum_hw_{hj}y_h$

   输出层输出：$z_j = sigmoid(net_j)$

3. 计算误差： $E = \dfrac{1}{2}\sum_{j=1}^{3}(t_j-z_j)^2$

#### 3.2.4 反向传播

- 输出层梯度：
- 隐含层梯度：
- 输出层权重更新：$w_{hj} ← w_{hj} − η⋅δ_j ⋅ y_h$
- 隐含层权重更新：$w_{ih} ← w_{ih} − η⋅δ_h ⋅ x_i$
- 两种权重更新方式：
  - 单样本更新：每输入一个样本就立即更新权重 
  - 批量更新：积累所有样本的梯度，每个epoch结束后同一更新权重



## 4 结果分析

### 4.1 实验参数

|      |      |      |
| ---- | ---- | ---- |
|      |      |      |
|      |      |      |
|      |      |      |

### 4.2 结论分析

隐含层不同节点数目对训练精度的影响

不同梯度更新步长对训练的影响

网络结构固定的情况下，绘制出目标函数随着贴袋部署增加的变化曲线