# 模式识别实验报告

**姓　　名**：范红乐  
**班　　级**：计算机学院20250718班  
**学　　号**：2025E8007382043

## 1 实验概述

### 1.1 实验内容

1. 实现两个通用的三层前向神经网络反向传播算法程序，一个采用批量方式更新权重，另一个采用单样本方式更新权重
2. 隐含层节点激励函数：双曲正切函数；输出层激励函数：Sigmoid函数；目标函数：平方误差准则函数

### 1.2 实验数据

<img src="E:\Typora\Typora\coding-study\image-20251229204817913.png" alt="image-20251229204817913" style="zoom: 67%;" />



## 2 算法实现

### 2.1 网络结构



| 网络层 | 节点数        | 激活函数                                                     |
| ------ | ------------- | ------------------------------------------------------------ |
| 输入层 | 3（3个特征）  |                                                              |
| 隐含层 | a（数目可调） | 双曲正切： $f(x) = tanh(x) = \dfrac{e^x-e^{-x}}{e^x + e^{-x}}$ |
| 输出层 | 3（3个类别）  | Sigmoid： $f(x) = \dfrac{1}{1+e^{-x}}$                       |
|        | **损失函数**  | **平方误差**： $E = \dfrac{1}{2}\sum_{j=1}^{3}(t_j-z_j)^2$   |





### 2.2 算法实现

#### 2.2.1 前向传播

1. 隐含层输入：$net_h = \sum_iw_{ih}x_i$  	

   隐含层输出：$y_h = tanh(net_h)$

2. 输出层输入：$net_j = \sum_hw_{hj}y_h$

   输出层输出：$z_j = sigmoid(net_j)$

3. 计算误差： $E = \dfrac{1}{2}\sum_{j=1}^{3}(t_j-z_j)^2$

#### 3.2.2 反向传播

- 输出层？？：$δ_j = \dfrac{-∂ E}{∂ net^{k}_{j}} = f'(net^{k}_{j})(t^{k}_{j}-z^{k}_{j})$
- 隐含层？？：$δ_h = \dfrac{-∂ E}{∂ net^{k}_{h}} = f'(net^{k}_{h})\sum_{j}w_{hj}\delta^{k}_{j}$
- 输出层权重更新：$w_{hj} ← w_{hj} − η⋅δ_j ⋅ y_h$
- 隐含层权重更新：$w_{ih} ← w_{ih} − η⋅δ_h ⋅ x_i$
- 两种权重更新方式：
  - 单样本更新：每输入一个样本就立即更新权重 
  - 批量更新：积累所有样本的梯度，每个epoch结束后同一更新权重





## 3 实验设置

### 3.1 目录结构

```
├── main.py  
├── dta_loader.py		  # 数据预处理
|
├── exp1_hidden_size.py   # 实验1 隐含层节点数测试
├── exp2_learning_rate.py # 实验2 学习率测试
├── exp3_update_method.py # 实验3 单样本更新 vs 批量更新    
│
├── plot_results.py		# 结果绘图处理
└── imgs/				# 实验结果图
```

### 3.2 实验参数

- 数据预处理
- 训练集与测试集比例
- 实验参数配置
  - 权重初始化 使用简单随机初始化





## 4 结果分析

图1 不同隐含层节点数的性能对比

1. 隐含层不同节点数目对训练精度的影响
   - 隐含层节点太少时，能够提取以及保存的模式较少，获得的模式不足以概括样本的所有邮箱信息，得不到样本的特定规律，导致识别同样模式的新样本的能力较差，学习能力较差
   - 隐含层节点多，学习时间变长，神经网络的学习能力较强，能学习较多输入数据之间的隐含模式。
   - 隐含层节点过多，学习能力过强，可能把训练输入样本与输出数据无关的非规律性模式学习进来，大都是一些样本噪声，导致过拟合，降低了模型泛化能力。（表现是在训练数据集上误差极小，测试数据集上误差较大）
   - 隐含层节点个数取决于样本中蕴含规律的个数以及复杂程度
     - 可以将隐含层个数设置为超参数，使用验证及验证，选择在验证集中误差最小的作为神经网络的隐含层的节点个数
     - 或者或通过简单的经验设置公式来确定隐含层神经元个数 $l=\sqrt{m+n}+α$（m输入层节点个数，n输出层节点个数，α一般是1-10的常数）

图2 不同学习率对比的示意图

2. 不同梯度更新步长对训练的影响



3. 网络结构固定的情况下，绘制出目标函数随着迭代步数增加的变化曲线



